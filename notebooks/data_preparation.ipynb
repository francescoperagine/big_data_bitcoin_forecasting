{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "sys.dont_write_bytecode = True\n",
    "from src.utils.utils import *\n",
    "from src.features.build_features import *\n",
    "from src.visualization.visualize import *\n",
    "\n",
    "from src.utils.constants import *\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_PCA = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = [\n",
    "    EXTERNAL_DATA_PATH,\n",
    "    INTERIM_DATA_PATH,\n",
    "    PROCESSED_DATA_PATH,\n",
    "    os.path.dirname(GROUND_TRUTH_PATH),\n",
    "    os.path.dirname(GROUND_TRUTH_SUMMARY),\n",
    "    FIGURE_PATH\n",
    "]\n",
    "\n",
    "for path in paths:\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ground truth summary\n",
    "\n",
    "ground_truth_threshold = pd.read_parquet(GROUND_TRUTH_SUMMARY)\n",
    "\n",
    "gtt_df = pd.DataFrame(ground_truth_threshold.loc[0])\n",
    "gtt_df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ground truth\n",
    "print(\"Ground truth\")\n",
    "\n",
    "ground_truth = pd.read_parquet(GROUND_TRUTH_PATH)\n",
    "print(\"Length:\", len(ground_truth))\n",
    "\n",
    "ground_truth_features = [x for x in ground_truth.columns]\n",
    "print(\"Features\", ground_truth_features)\n",
    "\n",
    "# Ground truth null values\n",
    "\n",
    "ground_truth_null = ground_truth.loc[ground_truth['null'] == True]\n",
    "\n",
    "print(f\"Date range: {ground_truth['origin_time'].min()} - {ground_truth['origin_time'].max()}\")\n",
    "print(f\"Null values length: {len(ground_truth_null)}\")\n",
    "\n",
    "ground_truth = ground_truth.loc[ground_truth['null'] != True]\n",
    "print(f\"Ground truth new length: {len(ground_truth)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data cleaning\n",
    "\n",
    "ground_truth.drop(columns=['null', 'close', 'next_change'], inplace=True)\n",
    "ground_truth.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exchanges summary\n",
    "\n",
    "data = {}\n",
    "\n",
    "for exchange in EXCHANGES:\n",
    "\n",
    "    candles_path = os.path.join(EXTERNAL_DATA_PATH, f'BTC-{exchange}_candles.parquet')\n",
    "    orderbooks_path = os.path.join(EXTERNAL_DATA_PATH, f'BTC-{exchange}_orderbook.parquet')   \n",
    "        \n",
    "    # Candles and orderbooks views\n",
    "    data[(exchange, CANDLES)] = pd.read_parquet(candles_path)\n",
    "    data[(exchange, ORDERBOOK)] = pd.read_parquet(orderbooks_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Null values summary\n",
    "\n",
    "candles_summary = []\n",
    "orderbooks_summary = []\n",
    "\n",
    "for (exchange, data_type), df in data.items():\n",
    "    if data_type == CANDLES:\n",
    "        dataset_df_summary = get_dataframe_null_summary(df, exchange)\n",
    "        candles_summary.append(dataset_df_summary)\n",
    "    elif data_type == ORDERBOOK:\n",
    "        dataset_df_summary = get_dataframe_null_summary(df, exchange)\n",
    "        orderbooks_summary.append(dataset_df_summary)\n",
    "\n",
    "display(pd.DataFrame(candles_summary))\n",
    "display(pd.DataFrame(orderbooks_summary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean null values\n",
    "\n",
    "for (exchange, data_type), df in data.items():\n",
    "    df = df.loc[df['null'] != True]\n",
    "    data[(exchange, data_type)] = df.drop(columns=['null'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exchange unified view\n",
    "\n",
    "for exchange in EXCHANGES:  \n",
    "    data[(exchange, UNIFIED)] = merge_datasets(data[(exchange, CANDLES)], data[(exchange, ORDERBOOK)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show data\n",
    "\n",
    "for (exchange, data_type), df in data.items():\n",
    "    display(f\"{exchange} {data_type}\", df.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add lag and time features, remove null values\n",
    "\n",
    "views = {}\n",
    "\n",
    "for (exchange, data_type), df in data.items():\n",
    "\n",
    "    # Add lag features\n",
    "    if data_type != ORDERBOOK:                    \n",
    "        df = add_lag_features(df, LAGS)\n",
    "\n",
    "    # Add time features\n",
    "    views[(exchange, data_type)] = add_time_features(df)\n",
    "    print(f\"Exchange: {exchange} {data_type} - old length: {len(df)}, new length: {len(views[(exchange, data_type)])}, null values: {len(df) - len(views[(exchange, data_type)])}\")\n",
    "\n",
    "    display(views[(exchange, data_type)].head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter and rename columns for the 'unified' datasets\n",
    "unified_data = {}\n",
    "for exchange in EXCHANGES:\n",
    "    df = views[(exchange, UNIFIED)]\n",
    "    df = df.rename(columns={col: f\"{col}_{exchange}\" if col != 'origin_time' else col for col in df.columns})\n",
    "    unified_data[exchange] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL = 'ALL'\n",
    "\n",
    "# Perform an inner join on 'origin_time'\n",
    "views[(ALL, UNIFIED)] = reduce(lambda left, right: pd.merge(left, right, on='origin_time', how='inner'), unified_data.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(views.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale data\n",
    "\n",
    "scaled_data = {}\n",
    "\n",
    "for (exchange, data_type), df in views.items():\n",
    "    \n",
    "    # Scale data\n",
    "    \n",
    "    df_no_time = df.drop(columns=['origin_time'])\n",
    "    scaled_data[(exchange, data_type)] = standard_scale(df_no_time)\n",
    "    scaled_data[(exchange, data_type)]['origin_time'] = df['origin_time']\n",
    "\n",
    "    display(scaled_data[(exchange, data_type)].head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate data correlation\n",
    "\n",
    "for (exchange, data_type), df in scaled_data.items():\n",
    "    evaluate_correlation(df, exchange, data_type, CORRELATION_THRESHOLD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA and explained variance\n",
    "\n",
    "pca_data = {}\n",
    "pca_loadings = {}\n",
    "\n",
    "for (exchange, data_type), df in scaled_data.items():\n",
    "\n",
    "    # Deattach time feature for computing PCA\n",
    "    no_time = df.drop(columns=['origin_time'])\n",
    "\n",
    "    pca_df, explained_variance, cumulative_variance, loadings = compute_pca(no_time, PCA_VARIANCE_THRESHOLD)\n",
    "\n",
    "    # And reattach it for merging it with GT later\n",
    "    pca_df['origin_time'] = df['origin_time'].values\n",
    "\n",
    "    pca_data[(exchange, data_type)] = pca_df\n",
    "    pca_loadings[(exchange, data_type)] = loadings\n",
    "\n",
    "    pd.DataFrame.to_parquet(pca_data[(exchange, data_type)], os.path.join(INTERIM_DATA_PATH, f\"{exchange}_{data_type}_pca_data.parquet\"))\n",
    "\n",
    "    plot_explained_variance(data_type, exchange, explained_variance, cumulative_variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA loadings heatmap\n",
    "\n",
    "for (exchange, data_type), df in pca_loadings.items():\n",
    "    plot_loadings_heatmap(data_type, exchange, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge datasets with ground truth before training\n",
    "\n",
    "merged_data = {}\n",
    "\n",
    "for (exchange, data_type), df in pca_data.items():\n",
    "    merged_data[(exchange, data_type)] = merge_datasets(df, ground_truth)\n",
    "    pd.DataFrame.to_parquet(merged_data[(exchange, data_type)], os.path.join(INTERIM_DATA_PATH, f\"{exchange}_{data_type}_merged.parquet\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
