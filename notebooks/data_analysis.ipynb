{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import pandas as pd\n",
    "\n",
    "src_path = os.path.abspath(os.path.join('..'))\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)\n",
    "\n",
    "sys.dont_write_bytecode = True\n",
    "from src.utils.utils import *\n",
    "from src.features.build_features import *\n",
    "from src.models.predict_model import *\n",
    "from src.models.train_model import *\n",
    "from src.visualization.visualize import *\n",
    "\n",
    "from src.utils.constants import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = [\n",
    "    EXTERNAL_DATA_PATH,\n",
    "    INTERIM_DATA_PATH,\n",
    "    PROCESSED_DATA_PATH,\n",
    "    os.path.dirname(GROUND_TRUTH_PATH),\n",
    "    os.path.dirname(GROUND_TRUTH_SUMMARY),\n",
    "    FIGURE_PATH\n",
    "]\n",
    "\n",
    "for path in paths:\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ground truth summary\n",
    "\n",
    "ground_truth_threshold = pd.read_parquet(GROUND_TRUTH_SUMMARY)\n",
    "\n",
    "gtt_df = pd.DataFrame(ground_truth_threshold.loc[0])\n",
    "gtt_df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ground truth\n",
    "print(\"Ground truth\")\n",
    "\n",
    "ground_truth = pd.read_parquet(GROUND_TRUTH_PATH)\n",
    "print(\"Length:\", len(ground_truth))\n",
    "\n",
    "ground_truth_features = [x for x in ground_truth.columns]\n",
    "print(\"Features\", ground_truth_features)\n",
    "\n",
    "# Ground truth null values\n",
    "\n",
    "ground_truth_null = ground_truth.loc[ground_truth['null'] == True]\n",
    "\n",
    "print(f\"Date range: {ground_truth['origin_time'].min()} - {ground_truth['origin_time'].max()}\")\n",
    "print(f\"Null values length: {len(ground_truth_null)}\")\n",
    "\n",
    "ground_truth = ground_truth.loc[ground_truth['null'] != True]\n",
    "print(f\"Ground truth new length: {len(ground_truth)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exchanges summary\n",
    "\n",
    "data = {}\n",
    "\n",
    "summary_data = []\n",
    "\n",
    "for exchange in EXCHANGES:\n",
    "    candles_path = os.path.join(EXTERNAL_DATA_PATH, f'BTC-{exchange}_candles.parquet')\n",
    "    orderbooks_path = os.path.join(EXTERNAL_DATA_PATH, f'BTC-{exchange}_orderbook.parquet')\n",
    "    \n",
    "    data[(CANDLES, exchange)] = pd.read_parquet(candles_path)\n",
    "    data[(ORDERBOOKS, exchange)] = pd.read_parquet(orderbooks_path)\n",
    "    \n",
    "    summary_data.append({\n",
    "        'Exchange': exchange,\n",
    "        'Candles Length': len(data[(CANDLES, exchange)]),\n",
    "        'Candles Date Range Start': data[(CANDLES, exchange)]['origin_time'].min(),\n",
    "        'Candles Date Range End': data[(CANDLES, exchange)]['origin_time'].max(),\n",
    "        'Orderbook Length': len(data[(ORDERBOOKS, exchange)]),\n",
    "        'Orderbook Date Range Start': data[(ORDERBOOKS, exchange)]['origin_time'].min(),\n",
    "        'Orderbook Date Range End': data[(ORDERBOOKS, exchange)]['origin_time'].max(),\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "display(summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets features\n",
    "\n",
    "first_candles_key = list(data.keys())[0]\n",
    "first_orderbooks_key = list(data.keys())[1]\n",
    "\n",
    "candles_fetures = get_features(data[first_candles_key])\n",
    "print(f\"Candles features len:\\t{len(candles_fetures)}\\n{candles_fetures}\")\n",
    "orderbook_features = get_features(data[first_orderbooks_key])\n",
    "print(f\"Orderbooks features len:\\t{len(orderbook_features)}\\n{orderbook_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Null values summary\n",
    "\n",
    "candles_summary = []\n",
    "orderbooks_summary = []\n",
    "\n",
    "for (data_type, exchange), df in data.items():\n",
    "    if data_type == CANDLES:\n",
    "        dataset_df_summary = get_dataframe_null_summary(df, exchange)\n",
    "        candles_summary.append(dataset_df_summary)\n",
    "    elif data_type == ORDERBOOKS:\n",
    "        dataset_df_summary = get_dataframe_null_summary(df, exchange)\n",
    "        orderbooks_summary.append(dataset_df_summary)\n",
    "\n",
    "display(pd.DataFrame(candles_summary))\n",
    "display(pd.DataFrame(orderbooks_summary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_data = {}\n",
    "\n",
    "for (data_type, exchange), df in data.items():\n",
    "    \n",
    "    # Remove null values, drop null feature\n",
    "\n",
    "    data[(data_type, exchange)] = df.loc[df['null'] != True]\n",
    "    data[(data_type, exchange)] = df.drop(columns=['null'])\n",
    "    \n",
    "    print(f\"Exchange: {exchange} - new {data_type} length: {len(df)}\")\n",
    "\n",
    "    # Scale data\n",
    "    \n",
    "    scaled_data[(data_type, exchange)] = data[(data_type, exchange)].drop(columns=['origin_time'])\n",
    "    scaled_data[(data_type, exchange)] = standard_scale(scaled_data[(data_type, exchange)])\n",
    "\n",
    "    pd.DataFrame.to_parquet(scaled_data[(data_type, exchange)], os.path.join(INTERIM_DATA_PATH, f'{exchange}_{data_type}_scaled.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix\n",
    "\n",
    "for (data_type, exchange), df in scaled_data.items():\n",
    "    correlation_matrix = df.corr()\n",
    "    \n",
    "    plot_correlation_matrix(data_type, exchange, correlation_matrix)\n",
    "\n",
    "    highly_correlated_pairs = correlation_matrix.unstack().sort_values(kind=\"quicksort\", ascending=False)\n",
    "    highly_correlated_pairs = highly_correlated_pairs[(highly_correlated_pairs != 1) & (highly_correlated_pairs > CORRELATION_THRESHOLD)]\n",
    "\n",
    "    print(\"Highly correlated pairs:\")\n",
    "    display(pd.DataFrame(highly_correlated_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA and explained variance\n",
    "\n",
    "pca = {}\n",
    "pca_data = {}\n",
    "\n",
    "for (data_type, exchange), df in data.items():\n",
    "    pca[(data_type, exchange)], explained_variance, cumulative_variance = perform_pca(scaled_data[(data_type, exchange)], PCA_VARIANCE_THRESHOLD)\n",
    "    pca_data[(data_type, exchange)] = {}\n",
    "\n",
    "    pca_transformed = pca[(data_type, exchange)].transform(scaled_data[(data_type, exchange)])\n",
    "    pca_transformed_df = pd.DataFrame(pca_transformed)\n",
    "    pca_transformed_df['origin_time'] = df['origin_time'].values\n",
    "    pca_data[(data_type, exchange)] = pca_transformed_df\n",
    "\n",
    "    pd.DataFrame.to_parquet(pca_data[(data_type, exchange)], os.path.join(INTERIM_DATA_PATH, f\"{exchange}_{data_type}_pca_data.parquet\"))\n",
    "\n",
    "    plot_pca_variance(data_type, exchange, explained_variance, cumulative_variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Candles matrices\n",
    "\n",
    "These matrices primarily includes various market indicators and technical analysis features such as SMA (Simple Moving Average), EMA (Exponential Moving Average), RSI (Relative Strength Index), MACD (Moving Average Convergence Divergence), etc.\n",
    "#### There are high correlations among similar indicators\n",
    "There's a visible high correlation among similar types of indicators, especially among different periods of moving averages (SMA, EMA). This is expected as these indicators are derived from the price and tend to move together.\n",
    "\n",
    "#### Potential Redundancy\n",
    "High correlations (close to 1) suggest redundancy among features. For instance, SMA and EMA values that are calculated over similar time frames may provide overlapping information which could be redundant in predictive modeling.\n",
    "\n",
    "#### Diverse Relationships\n",
    "Some features show moderate to low correlations, suggesting that they capture different aspects of the market behavior. These features can add valuable diversity to models.\n",
    "\n",
    "## Orderbooks matrices\n",
    "\n",
    "These matrices represents features related to the sizes of bids and asks at different levels in an order book. The periodic patterns indicate:\n",
    "\n",
    "#### Alternating High/Low Correlation\n",
    "The alternating pattern of high and low correlations suggests a structured dependency in order sizes, possibly alternating between bid and ask sizes or different levels of depth in the order book.\n",
    "\n",
    "#### Structured Market Dynamics\n",
    "The structured high correlations (red squares) alternating with lower correlations might indicate typical behaviors in how bids and asks are placed and modified in relation to each other. These patterns might reflect strategic placing/removal of orders at certain levels, influenced by market conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA Loadings: how the original features contribute to the principal components\n",
    "\n",
    "pca_loadings = {}\n",
    "\n",
    "for (data_type, exchange), df in scaled_data.items():\n",
    "    pca_loadings[(data_type, exchange)] = compute_loadings(pca[(data_type, exchange)], scaled_data[(data_type, exchange)])\n",
    "\n",
    "    pd.DataFrame.to_parquet(pca_loadings[(data_type, exchange)], os.path.join(INTERIM_DATA_PATH, f'{exchange}_{data_type}_pca_loadings.parquet'))\n",
    "    \n",
    "    plot_loadings_heatmap(data_type, exchange, pca_loadings[(data_type, exchange)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge ground truth with candles and orderbooks\n",
    "merged_df = {}\n",
    "\n",
    "for (data_type, exchange), df in data.items():\n",
    "    merged_df[(data_type, exchange)] = {}\n",
    "    merged_df[(data_type, exchange)]['full'] = pd.merge(ground_truth[['origin_time', 'label']], df, on='origin_time', how='inner')\n",
    "\n",
    "    cols_to_drop = ['origin_time', 'label']\n",
    "\n",
    "    merged_df[(data_type, exchange)]['X'] = merged_df[(data_type, exchange)]['full'].drop(cols_to_drop, axis=1)\n",
    "    merged_df[(data_type, exchange)]['y'] = merged_df[(data_type, exchange)]['full']['label']\n",
    "\n",
    "    pd.DataFrame.to_parquet(merged_df[(data_type, exchange)]['full'], os.path.join(INTERIM_DATA_PATH, f'{exchange}_{data_type}_merged.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "information_gain = {}\n",
    "\n",
    "for (data_type, exchange), df in merged_df.items():\n",
    "\n",
    "    # Inner merge ground truth with data on origin_time\n",
    "    information_gain[(data_type, exchange)] = get_information_gain(df['X'], df['y'])\n",
    "\n",
    "    pd.DataFrame.to_parquet(information_gain[(data_type, exchange)], os.path.join(INTERIM_DATA_PATH, f\"{exchange}_{data_type}_information_gain.parquet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_features = {}\n",
    "\n",
    "for (data_type, exchange), df in pca_loadings.items():\n",
    "    best_features[(data_type, exchange)] = compare_features_scores(df, information_gain[(data_type, exchange)])\n",
    "    display(best_features[(data_type, exchange)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (data_type, exchange), df in best_features.items():\n",
    "    print(best_features[(data_type, exchange)].sort_values(by='Combined_Scores', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (data_type, exchange), df in pca_loadings.items():\n",
    "    plot_histogram_density(data_type, exchange, best_features[(data_type, exchange)], ['Loadings_Norm', 'Information_Gain', 'Combined_Scores'], ['blue', 'red', 'green'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (data_type, exchange), df in best_features.items():\n",
    "    percentile_90 = np.percentile(df['Combined_Scores'], 80)\n",
    "\n",
    "    # Select features with scores above this percentile\n",
    "    selected_features = df[df['Combined_Scores'] >= percentile_90]\n",
    "\n",
    "    # Count the number of selected features\n",
    "    num_selected_features = selected_features.shape[0]\n",
    "    print(f\"{exchange}-{data_type} - Number of selected features: {num_selected_features}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
